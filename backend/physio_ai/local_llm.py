# -*- coding: utf-8 -*-
"""
Local LLM Module for Physio Capture
====================================

Este m√≥dulo gerencia a integra√ß√£o com o modelo de linguagem local (LLM)
usando llama-cpp-python. O modelo DeepSeek-R1-Distill-Qwen-1.5B-Q8_0 √©
carregado uma √∫nica vez (singleton) e reutilizado para todas as requisi√ß√µes.

Uso:
    from physio_ai import ask_physio_assistant
    resposta = ask_physio_assistant("Como cadastrar um paciente?")
"""

import os
import re
import logging
from functools import lru_cache
from pathlib import Path

# Configura√ß√£o de logging
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURA√á√ïES DO MODELO
# ============================================================================

# Caminho padr√£o do modelo (relativo ao diret√≥rio backend)
DEFAULT_MODEL_PATH = Path(__file__).resolve().parent.parent / "models_llm" / "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf"

# Obt√©m o caminho do modelo via vari√°vel de ambiente ou usa o padr√£o
MODEL_PATH = os.environ.get("PHYSIO_LLM_MODEL", str(DEFAULT_MODEL_PATH))

# Par√¢metros do modelo
MODEL_CONFIG = {
    "n_ctx": 4096,           # Tamanho do contexto
    "n_threads": os.cpu_count() or 4,  # N√∫mero de threads (baseado em CPUs)
    "verbose": False,        # Desativa logs verbosos em produ√ß√£o
}

# Par√¢metros de gera√ß√£o
GENERATION_CONFIG = {
    "temperature": 0.5,      # Temperatura moderada para respostas variadas mas coerentes
    "max_tokens": 1024,      # M√°ximo de tokens na resposta
    "top_p": 0.9,            # Nucleus sampling
    "top_k": 40,             # Top-k sampling
}

# ============================================================================
# SYSTEM PROMPT
# ============================================================================

SYSTEM_PROMPT = """Voc√™ √© o assistente oficial do sistema Physio Capture, uma plataforma de gest√£o cl√≠nica para fisioterapia.

REGRAS OBRIGAT√ìRIAS:
1. SEMPRE responda em portugu√™s do Brasil.
2. NUNCA mostre seu processo de pensamento ou tags <think>.
3. Responda diretamente √† pergunta do usu√°rio.
4. Seja claro, objetivo e did√°tico.
5. Use emojis para tornar a resposta amig√°vel.

FUNCIONALIDADES DO SISTEMA:

üìã PRONTU√ÅRIO ELETR√îNICO:
- Cadastro de pacientes com dados pessoais e foto
- Hist√≥rico cl√≠nico completo
- Evolu√ß√£o do tratamento

üìÖ AGENDAMENTO:
- Marca√ß√£o de consultas e sess√µes
- Calend√°rio de disponibilidade
- Notifica√ß√µes

üì∑ DIGITALIZA√á√ÉO DE DOCUMENTOS:
- Upload de documentos (PDF, imagens)
- OCR para extrair texto automaticamente
- Organiza√ß√£o por categorias

üìä RELAT√ìRIOS:
- Relat√≥rios cl√≠nicos
- Relat√≥rios administrativos
- Estat√≠sticas

EXEMPLOS DE RESPOSTAS:
- Se perguntarem sobre cadastro de paciente: explique os passos no menu Pacientes > Novo Paciente
- Se perguntarem sobre documentos: explique o fluxo de digitaliza√ß√£o e OCR
- Se perguntarem sobre prontu√°rio: explique as abas e campos dispon√≠veis"""


# ============================================================================
# FUN√á√ïES AUXILIARES
# ============================================================================

def clean_response(text: str) -> str:
    """
    Limpa a resposta do modelo DeepSeek-R1, removendo tags de pensamento.
    
    O modelo DeepSeek-R1 adiciona se√ß√µes <think>...</think> com seu processo
    de racioc√≠nio. Esta fun√ß√£o remove essas se√ß√µes e retorna apenas a resposta final.
    
    Args:
        text: Texto da resposta do modelo.
    
    Returns:
        str: Texto limpo, apenas com a resposta final.
    """
    if not text:
        return ""
    
    # Se houver </think>, pega apenas o conte√∫do DEPOIS dele
    if '</think>' in text.lower():
        # Encontra a posi√ß√£o do </think> (case insensitive)
        match = re.search(r'</think>', text, re.IGNORECASE)
        if match:
            text = text[match.end():]
    
    # Remove qualquer tag <think> de abertura que sobrou
    text = re.sub(r'<think>', '', text, flags=re.IGNORECASE)
    text = re.sub(r'<thinking>', '', text, flags=re.IGNORECASE)
    
    # Remove tags de fechamento √≥rf√£s
    text = re.sub(r'</think>', '', text, flags=re.IGNORECASE)
    text = re.sub(r'</thinking>', '', text, flags=re.IGNORECASE)
    
    # Remove blocos completos <think>...</think> que ainda possam existir
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r'<thinking>.*?</thinking>', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # Remove espa√ßos extras e quebras de linha no in√≠cio/fim
    text = text.strip()
    
    # Remove m√∫ltiplas quebras de linha consecutivas
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text


# ============================================================================
# FUN√á√ïES PRINCIPAIS
# ============================================================================

@lru_cache(maxsize=1)
def get_llm():
    """
    Carrega e retorna a inst√¢ncia do modelo LLM.
    
    Usa @lru_cache para garantir que o modelo seja carregado apenas uma vez
    (padr√£o singleton), evitando recarregamentos e melhorando a performance.
    
    Returns:
        llama_cpp.Llama: Inst√¢ncia do modelo carregado.
    
    Raises:
        FileNotFoundError: Se o arquivo do modelo n√£o existir.
        ImportError: Se llama-cpp-python n√£o estiver instalado.
    """
    # Importa√ß√£o lazy para evitar erros se a biblioteca n√£o estiver instalada
    try:
        from llama_cpp import Llama
    except ImportError as e:
        logger.error("llama-cpp-python n√£o est√° instalado. Execute: pip install llama-cpp-python")
        raise ImportError(
            "A biblioteca llama-cpp-python √© necess√°ria. "
            "Instale com: pip install llama-cpp-python"
        ) from e
    
    # Verifica se o arquivo do modelo existe
    model_path = Path(MODEL_PATH)
    if not model_path.exists():
        error_msg = (
            f"Arquivo do modelo n√£o encontrado: {model_path}\n"
            f"Por favor, baixe o modelo DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf "
            f"e coloque-o na pasta: {model_path.parent}"
        )
        logger.error(error_msg)
        raise FileNotFoundError(error_msg)
    
    logger.info(f"Carregando modelo LLM de: {model_path}")
    logger.info(f"Configura√ß√µes: n_ctx={MODEL_CONFIG['n_ctx']}, n_threads={MODEL_CONFIG['n_threads']}")
    
    # Carrega o modelo
    llm = Llama(
        model_path=str(model_path),
        n_ctx=MODEL_CONFIG["n_ctx"],
        n_threads=MODEL_CONFIG["n_threads"],
        verbose=MODEL_CONFIG["verbose"],
        chat_format="chatml",  # Formato de chat compat√≠vel com DeepSeek
    )
    
    logger.info("Modelo LLM carregado com sucesso!")
    return llm


def ask_physio_assistant(message: str) -> str:
    """
    Envia uma pergunta ao assistente Physio Capture e retorna a resposta.
    
    Args:
        message: A pergunta do usu√°rio sobre o sistema Physio Capture.
    
    Returns:
        str: A resposta gerada pelo modelo de IA.
    
    Raises:
        ValueError: Se a mensagem estiver vazia.
        Exception: Se ocorrer erro na gera√ß√£o da resposta.
    """
    # Valida√ß√£o de entrada
    if not message or not message.strip():
        raise ValueError("A mensagem n√£o pode estar vazia.")
    
    message = message.strip()
    logger.info(f"Processando pergunta: {message[:100]}...")
    
    # Obt√©m a inst√¢ncia do modelo
    llm = get_llm()
    
    # Monta a lista de mensagens para o chat
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": message},
    ]
    
    # Gera a resposta
    try:
        response = llm.create_chat_completion(
            messages=messages,
            temperature=GENERATION_CONFIG["temperature"],
            max_tokens=GENERATION_CONFIG["max_tokens"],
            top_p=GENERATION_CONFIG["top_p"],
            top_k=GENERATION_CONFIG["top_k"],
        )
        
        # Extrai o texto da resposta
        answer = response["choices"][0]["message"]["content"]
        
        # Limpa a resposta (remove tags de pensamento e espa√ßos extras)
        answer = clean_response(answer)
        
        logger.info(f"Resposta gerada com sucesso ({len(answer)} caracteres)")
        return answer
        
    except Exception as e:
        logger.error(f"Erro ao gerar resposta: {str(e)}")
        raise


def check_model_status() -> dict:
    """
    Verifica o status do modelo e retorna informa√ß√µes de diagn√≥stico.
    
    Returns:
        dict: Dicion√°rio com informa√ß√µes sobre o status do modelo.
    """
    model_path = Path(MODEL_PATH)
    
    status = {
        "model_path": str(model_path),
        "model_exists": model_path.exists(),
        "model_size_mb": None,
        "llama_cpp_installed": False,
        "model_loaded": False,
        "n_threads": MODEL_CONFIG["n_threads"],
        "n_ctx": MODEL_CONFIG["n_ctx"],
    }
    
    # Verifica tamanho do modelo
    if model_path.exists():
        status["model_size_mb"] = round(model_path.stat().st_size / (1024 * 1024), 2)
    
    # Verifica se llama-cpp-python est√° instalado
    try:
        import llama_cpp
        status["llama_cpp_installed"] = True
        status["llama_cpp_version"] = getattr(llama_cpp, "__version__", "unknown")
    except ImportError:
        pass
    
    # Verifica se o modelo j√° foi carregado
    try:
        # Tenta acessar o cache do get_llm
        if get_llm.cache_info().hits > 0 or get_llm.cache_info().currsize > 0:
            status["model_loaded"] = True
    except Exception:
        pass
    
    return status
